{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraday Strategy, Part 2: Model Training & Signal Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the high-quality NASDAQ100 minute-bar trade-and-quote data generously provided by [Algoseek](https://www.algoseek.com/) (available [here](https://www.algoseek.com/ml4t-book-data.html)) and use the features engineered in the last notebook to train gradient boosting model that predicts the returns for the NASDAQ100 stocks over the next 1-minute bar. \n",
    "\n",
    "> Note that we will assume throughout that we can always buy (sell) at the first (last) trade price for a given bar at no cost and without market impact. This does certainly not reflect market reality, and is rather due to the challenges of simulating a trading strategy at this much higher intraday frequency in a realistic manner using open-source tools.\n",
    "\n",
    "Note also that this section has slightly changed from the version published in the book to permit replication using the Algoseek data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /home/jim/.cache/uv/archive-v0/aWg7grEuJ9_NpgJY_Vt8J/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/jim/.cache/uv/archive-v0/aWg7grEuJ9_NpgJY_Vt8J/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/jim/.cache/uv/archive-v0/aWg7grEuJ9_NpgJY_Vt8J/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/jim/.cache/uv/archive-v0/aWg7grEuJ9_NpgJY_Vt8J/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/jim/.cache/uv/archive-v0/aWg7grEuJ9_NpgJY_Vt8J/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --install-option\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%sudo apt install cmake\n",
    "%pip install \"lightgbm==3.3.5\" --no-binary lightgbm --config-setting=cmake.define.USE_CUDA=ON \n",
    "%pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.cupy.dev/en/stable/install.html#installing-cupy\n",
    "%pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:12.793759Z",
     "start_time": "2021-01-20T05:06:12.791425Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:13.496235Z",
     "start_time": "2021-01-20T05:06:12.795057Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:13.499714Z",
     "start_time": "2021-01-20T05:06:13.497313Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    \"\"\"Return a formatted time string 'HH:MM:SS\n",
    "    based on a numeric time() value\"\"\"\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T17:54:54.604886Z",
     "start_time": "2021-01-20T17:54:54.602867Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "idx = pd.IndexSlice\n",
    "deciles = np.arange(.1, 1, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:13.517935Z",
     "start_time": "2021-01-20T05:06:13.510733Z"
    }
   },
   "outputs": [],
   "source": [
    "# where we stored the features engineered in the previous notebook\n",
    "data_store = '../data/nasdaq100/algoseek.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:13.526099Z",
     "start_time": "2021-01-20T05:06:13.519257Z"
    }
   },
   "outputs": [],
   "source": [
    "# where we'll store the model results\n",
    "result_store = 'results/intra_day.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:13.534318Z",
     "start_time": "2021-01-20T05:06:13.528377Z"
    }
   },
   "outputs": [],
   "source": [
    "# here we save the trained models\n",
    "model_path = Path('models/intraday')\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:22.192145Z",
     "start_time": "2021-01-20T05:06:13.535527Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/io/pytables.py:452\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/io/pytables.py:906\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# create the iterator\u001b[39;00m\n\u001b[1;32m    893\u001b[0m it \u001b[38;5;241m=\u001b[39m TableIterator(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m     auto_close\u001b[38;5;241m=\u001b[39mauto_close,\n\u001b[1;32m    904\u001b[0m )\n\u001b[0;32m--> 906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/io/pytables.py:2029\u001b[0m, in \u001b[0;36mTableIterator.get_result\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     where \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;66;03m# directly return the result\u001b[39;00m\n\u001b[0;32m-> 2029\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/io/pytables.py:890\u001b[0m, in \u001b[0;36mHDFStore.select.<locals>.func\u001b[0;34m(_start, _stop, _where)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(_start, _stop, _where):\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/io/pytables.py:3301\u001b[0m, in \u001b[0;36mBlockManagerFixed.read\u001b[0;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[1;32m   3298\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m   3300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3301\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m   3303\u001b[0m         \u001b[38;5;66;03m# with CoW, concat ignores the copy keyword. Here, we still want\u001b[39;00m\n\u001b[1;32m   3304\u001b[0m         \u001b[38;5;66;03m# to copy to enforce optimized column-major layout\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/internals/concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/internals/concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[0;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[1;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    221\u001b[0m             axes[i],\n\u001b[1;32m    222\u001b[0m             indexers[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[0;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_hdf(data_store, 'model_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:23.416212Z",
     "start_time": "2021-01-20T05:06:22.193256Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:27.993134Z",
     "start_time": "2021-01-20T05:06:23.417369Z"
    }
   },
   "outputs": [],
   "source": [
    "data.sample(frac=.1).describe(percentiles=np.arange(.1, 1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:27.999759Z",
     "start_time": "2021-01-20T05:06:27.994007Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultipleTimeSeriesCV:\n",
    "    \"\"\"Generates tuples of train_idx, test_idx pairs\n",
    "    Assumes the MultiIndex contains levels 'symbol' and 'date'\n",
    "    purges overlapping outcomes\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits=3,\n",
    "                 train_period_length=126,\n",
    "                 test_period_length=21,\n",
    "                 lookahead=None,\n",
    "                 date_idx='date',\n",
    "                 shuffle=False):\n",
    "        self.n_splits = n_splits\n",
    "        self.lookahead = lookahead\n",
    "        self.test_length = test_period_length\n",
    "        self.train_length = train_period_length\n",
    "        self.shuffle = shuffle\n",
    "        self.date_idx = date_idx\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        unique_dates = X.index.get_level_values(self.date_idx).unique()\n",
    "        days = sorted(unique_dates, reverse=True)\n",
    "        split_idx = []\n",
    "        for i in range(self.n_splits):\n",
    "            test_end_idx = i * self.test_length\n",
    "            test_start_idx = test_end_idx + self.test_length\n",
    "            train_end_idx = test_start_idx + self.lookahead - 1\n",
    "            train_start_idx = train_end_idx + self.train_length + self.lookahead - 1\n",
    "            split_idx.append([train_start_idx, train_end_idx,\n",
    "                              test_start_idx, test_end_idx])\n",
    "\n",
    "        dates = X.reset_index()[[self.date_idx]]\n",
    "        for train_start, train_end, test_start, test_end in split_idx:\n",
    "\n",
    "            train_idx = dates[(dates[self.date_idx] > days[train_start])\n",
    "                              & (dates[self.date_idx] <= days[train_end])].index\n",
    "            test_idx = dates[(dates[self.date_idx] > days[test_start])\n",
    "                             & (dates[self.date_idx] <= days[test_end])].index\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(list(train_idx))\n",
    "            yield train_idx.to_numpy(), test_idx.to_numpy()\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:28.012433Z",
     "start_time": "2021-01-20T05:06:28.001098Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fi(model):\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(),\n",
    "                      index=model.feature_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:29.444688Z",
     "start_time": "2021-01-20T05:06:28.013407Z"
    }
   },
   "outputs": [],
   "source": [
    "data['stock_id'] = pd.factorize(data.index.get_level_values('ticker'), sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:29.447321Z",
     "start_time": "2021-01-20T05:06:29.445657Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricals = ['stock_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def ic_lgbm(preds: np.ndarray, train_data: np.ndarray):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], True # is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import cupyx.scipy.stats as cupy_stats\n",
    "\n",
    "# This doesn't work because CuPy doesn't have stats.spearmanr, or most other scipy.stats functions.\n",
    "def ic_lgbm(preds, train_data):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    # is_higher_better = True\n",
    "    preds_gpu = cupy.array(preds)\n",
    "    labels_gpu = cupy.array(train_data.get_label())\n",
    "    return 'ic', cpstats.spearmanr(preds_gpu, labels_gpu)[0], True\n",
    "\n",
    "cupy.cuda.runtime.getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "\n",
    "# Source from https://github.com/scipy/scipy/blob/main/scipy/stats/_mstats_basic.py\n",
    "# Inline Cython because these functions aren't .pyx in SciPy.\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import numpy.ma as ma\n",
    "from numpy.ma import masked, nomask\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "\n",
    "from scipy._lib._util import _rename_parameter, _contains_nan\n",
    "from scipy._lib._bunch import _make_tuple_bunch\n",
    "import scipy.special as special\n",
    "import scipy.stats._stats_py\n",
    "import scipy.stats._stats_py as _stats_py\n",
    "\n",
    "\n",
    "def _chk_asarray(a, axis):\n",
    "    # Always returns a masked array, raveled for axis=None\n",
    "    a = ma.asanyarray(a)\n",
    "    if axis is None:\n",
    "        a = ma.ravel(a)\n",
    "        outaxis = 0\n",
    "    else:\n",
    "        outaxis = axis\n",
    "    return a, outaxis\n",
    "\n",
    "\n",
    "def _ttest_finish(df, t, alternative):\n",
    "    \"\"\"Common code between all 3 t-test functions.\"\"\"\n",
    "    # We use ``stdtr`` directly here to preserve masked arrays\n",
    "\n",
    "    if alternative == 'less':\n",
    "        pval = special._ufuncs.stdtr(df, t)\n",
    "    elif alternative == 'greater':\n",
    "        pval = special._ufuncs.stdtr(df, -t)\n",
    "    elif alternative == 'two-sided':\n",
    "        pval = special._ufuncs.stdtr(df, -np.abs(t))*2\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be \"\n",
    "                         \"'less', 'greater' or 'two-sided'\")\n",
    "\n",
    "    if t.ndim == 0:\n",
    "        t = t[()]\n",
    "    if pval.ndim == 0:\n",
    "        pval = pval[()]\n",
    "\n",
    "    return t, pval\n",
    "\n",
    "\n",
    "def _find_repeats(arr):\n",
    "    # This function assumes it may clobber its input.\n",
    "    if len(arr) == 0:\n",
    "        return np.array(0, np.float64), np.array(0, np.intp)\n",
    "\n",
    "    # XXX This cast was previously needed for the Fortran implementation,\n",
    "    # should we ditch it?\n",
    "    arr = np.asarray(arr, np.float64).ravel()\n",
    "    arr.sort()\n",
    "\n",
    "    # Taken from NumPy 1.9's np.unique.\n",
    "    change = np.concatenate(([True], arr[1:] != arr[:-1]))\n",
    "    unique = arr[change]\n",
    "    change_idx = np.concatenate(np.nonzero(change) + ([arr.size],))\n",
    "    freq = np.diff(change_idx)\n",
    "    atleast2 = freq > 1\n",
    "    return unique[atleast2], freq[atleast2]\n",
    "\n",
    "\n",
    "def find_repeats(arr):\n",
    "    \"\"\"Find repeats in arr and return a tuple (repeats, repeat_count).\n",
    "\n",
    "    The input is cast to float64. Masked values are discarded.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : sequence\n",
    "        Input array. The array is flattened if it is not 1D.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    repeats : ndarray\n",
    "        Array of repeated values.\n",
    "    counts : ndarray\n",
    "        Array of counts.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from scipy.stats import mstats\n",
    "    >>> mstats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n",
    "    (array([2.]), array([4]))\n",
    "\n",
    "    In the above example, 2 repeats 4 times.\n",
    "\n",
    "    >>> mstats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n",
    "    (array([4., 5.]), array([2, 2]))\n",
    "\n",
    "    In the above example, both 4 and 5 repeat 2 times.\n",
    "\n",
    "    \"\"\"\n",
    "    # Make sure we get a copy. ma.compressed promises a \"new array\", but can\n",
    "    # actually return a reference.\n",
    "    compr = np.asarray(ma.compressed(arr), dtype=np.float64)\n",
    "    try:\n",
    "        need_copy = np.may_share_memory(compr, arr)\n",
    "    except AttributeError:\n",
    "        # numpy < 1.8.2 bug: np.may_share_memory([], []) raises,\n",
    "        # while in numpy 1.8.2 and above it just (correctly) returns False.\n",
    "        need_copy = False\n",
    "    if need_copy:\n",
    "        compr = compr.copy()\n",
    "    return _find_repeats(compr)\n",
    "\n",
    "\n",
    "def rankdata(data, axis=None, use_missing=False):\n",
    "    \"\"\"Returns the rank (also known as order statistics) of each data point\n",
    "    along the given axis.\n",
    "\n",
    "    If some values are tied, their rank is averaged.\n",
    "    If some values are masked, their rank is set to 0 if use_missing is False,\n",
    "    or set to the average rank of the unmasked values if use_missing is True.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : sequence\n",
    "        Input data. The data is transformed to a masked array\n",
    "    axis : {None,int}, optional\n",
    "        Axis along which to perform the ranking.\n",
    "        If None, the array is first flattened. An exception is raised if\n",
    "        the axis is specified for arrays with a dimension larger than 2\n",
    "    use_missing : bool, optional\n",
    "        Whether the masked values have a rank of 0 (False) or equal to the\n",
    "        average rank of the unmasked values (True).\n",
    "\n",
    "    \"\"\"\n",
    "    def _rank1d(data, use_missing=False):\n",
    "        n = data.count()\n",
    "        rk = np.empty(data.size, dtype=float)\n",
    "        idx = data.argsort()\n",
    "        rk[idx[:n]] = np.arange(1,n+1)\n",
    "\n",
    "        if use_missing:\n",
    "            rk[idx[n:]] = (n+1)/2.\n",
    "        else:\n",
    "            rk[idx[n:]] = 0\n",
    "\n",
    "        repeats = find_repeats(data.copy())\n",
    "        for r in repeats[0]:\n",
    "            condition = (data == r).filled(False)\n",
    "            rk[condition] = rk[condition].mean()\n",
    "        return rk\n",
    "\n",
    "    data = ma.array(data, copy=False)\n",
    "    if axis is None:\n",
    "        if data.ndim > 1:\n",
    "            return _rank1d(data.ravel(), use_missing).reshape(data.shape)\n",
    "        else:\n",
    "            return _rank1d(data, use_missing)\n",
    "    else:\n",
    "        return ma.apply_along_axis(_rank1d,axis,data,use_missing).view(ndarray)\n",
    "\n",
    "\n",
    "    ModeResult = namedtuple('ModeResult', ('mode', 'count'))\n",
    "\n",
    "\n",
    "\n",
    "def spearmanr(x, y=None, use_ties=True, axis=None, nan_policy='propagate',\n",
    "              alternative='two-sided'):\n",
    "    \"\"\"\n",
    "    Calculates a Spearman rank-order correlation coefficient and the p-value\n",
    "    to test for non-correlation.\n",
    "\n",
    "    The Spearman correlation is a nonparametric measure of the linear\n",
    "    relationship between two datasets. Unlike the Pearson correlation, the\n",
    "    Spearman correlation does not assume that both datasets are normally\n",
    "    distributed. Like other correlation coefficients, this one varies\n",
    "    between -1 and +1 with 0 implying no correlation. Correlations of -1 or\n",
    "    +1 imply a monotonic relationship. Positive correlations imply that\n",
    "    as `x` increases, so does `y`. Negative correlations imply that as `x`\n",
    "    increases, `y` decreases.\n",
    "\n",
    "    Missing values are discarded pair-wise: if a value is missing in `x`, the\n",
    "    corresponding value in `y` is masked.\n",
    "\n",
    "    The p-value roughly indicates the probability of an uncorrelated system\n",
    "    producing datasets that have a Spearman correlation at least as extreme\n",
    "    as the one computed from these datasets. The p-values are not entirely\n",
    "    reliable but are probably reasonable for datasets larger than 500 or so.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : 1D or 2D array_like, y is optional\n",
    "        One or two 1-D or 2-D arrays containing multiple variables and\n",
    "        observations. When these are 1-D, each represents a vector of\n",
    "        observations of a single variable. For the behavior in the 2-D case,\n",
    "        see under ``axis``, below.\n",
    "    use_ties : bool, optional\n",
    "        DO NOT USE.  Does not do anything, keyword is only left in place for\n",
    "        backwards compatibility reasons.\n",
    "    axis : int or None, optional\n",
    "        If axis=0 (default), then each column represents a variable, with\n",
    "        observations in the rows. If axis=1, the relationship is transposed:\n",
    "        each row represents a variable, while the columns contain observations.\n",
    "        If axis=None, then both arrays will be raveled.\n",
    "    nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
    "        Defines how to handle when input contains nan. 'propagate' returns nan,\n",
    "        'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
    "        values. Default is 'propagate'.\n",
    "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
    "        Defines the alternative hypothesis. Default is 'two-sided'.\n",
    "        The following options are available:\n",
    "\n",
    "        * 'two-sided': the correlation is nonzero\n",
    "        * 'less': the correlation is negative (less than zero)\n",
    "        * 'greater':  the correlation is positive (greater than zero)\n",
    "\n",
    "        .. versionadded:: 1.7.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : SignificanceResult\n",
    "        An object containing attributes:\n",
    "\n",
    "        statistic : float or ndarray (2-D square)\n",
    "            Spearman correlation matrix or correlation coefficient (if only 2\n",
    "            variables are given as parameters). Correlation matrix is square\n",
    "            with length equal to total number of variables (columns or rows) in\n",
    "            ``a`` and ``b`` combined.\n",
    "        pvalue : float\n",
    "            The p-value for a hypothesis test whose null hypothesis\n",
    "            is that two sets of data are linearly uncorrelated. See\n",
    "            `alternative` above for alternative hypotheses. `pvalue` has the\n",
    "            same shape as `statistic`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [CRCProbStat2000] section 14.7\n",
    "\n",
    "    \"\"\"\n",
    "    if not use_ties:\n",
    "        raise ValueError(\"`use_ties=False` is not supported in SciPy >= 1.2.0\")\n",
    "\n",
    "    # Always returns a masked array, raveled if axis=None\n",
    "    x, axisout = _chk_asarray(x, axis)\n",
    "    if y is not None:\n",
    "        # Deal only with 2-D `x` case.\n",
    "        y, _ = _chk_asarray(y, axis)\n",
    "        if axisout == 0:\n",
    "            x = ma.column_stack((x, y))\n",
    "        else:\n",
    "            x = ma.vstack((x, y))\n",
    "\n",
    "    if axisout == 1:\n",
    "        # To simplify the code that follow (always use `n_obs, n_vars` shape)\n",
    "        x = x.T\n",
    "\n",
    "    if nan_policy == 'omit':\n",
    "        x = ma.masked_invalid(x)\n",
    "\n",
    "    def _spearmanr_2cols(x):\n",
    "        # Mask the same observations for all variables, and then drop those\n",
    "        # observations (can't leave them masked, rankdata is weird).\n",
    "        x = ma.mask_rowcols(x, axis=0)\n",
    "        x = x[~x.mask.any(axis=1), :]\n",
    "\n",
    "        # If either column is entirely NaN or Inf\n",
    "        if not np.any(x.data):\n",
    "            res = scipy.stats._stats_py.SignificanceResult(np.nan, np.nan)\n",
    "            res.correlation = np.nan\n",
    "            return res\n",
    "\n",
    "        m = ma.getmask(x)\n",
    "        n_obs = x.shape[0]\n",
    "        dof = n_obs - 2 - int(m.sum(axis=0)[0])\n",
    "        if dof < 0:\n",
    "            raise ValueError(\"The input must have at least 3 entries!\")\n",
    "\n",
    "        # Gets the ranks and rank differences\n",
    "        x_ranked = rankdata(x, axis=0)\n",
    "        rs = ma.corrcoef(x_ranked, rowvar=False).data\n",
    "\n",
    "        # rs can have elements equal to 1, so avoid zero division warnings\n",
    "        with np.errstate(divide='ignore'):\n",
    "            # clip the small negative values possibly caused by rounding\n",
    "            # errors before taking the square root\n",
    "            t = rs * np.sqrt((dof / ((rs+1.0) * (1.0-rs))).clip(0))\n",
    "\n",
    "        t, prob = _ttest_finish(dof, t, alternative)\n",
    "\n",
    "        # For backwards compatibility, return scalars when comparing 2 columns\n",
    "        if rs.shape == (2, 2):\n",
    "            res = scipy.stats._stats_py.SignificanceResult(rs[1, 0],\n",
    "                                                           prob[1, 0])\n",
    "            res.correlation = rs[1, 0]\n",
    "            return res\n",
    "        else:\n",
    "            res = scipy.stats._stats_py.SignificanceResult(rs, prob)\n",
    "            res.correlation = rs\n",
    "            return res\n",
    "\n",
    "    # Need to do this per pair of variables, otherwise the dropped observations\n",
    "    # in a third column mess up the result for a pair.\n",
    "    n_vars = x.shape[1]\n",
    "    if n_vars == 2:\n",
    "        return _spearmanr_2cols(x)\n",
    "    else:\n",
    "        rs = np.ones((n_vars, n_vars), dtype=float)\n",
    "        prob = np.zeros((n_vars, n_vars), dtype=float)\n",
    "        for var1 in range(n_vars - 1):\n",
    "            for var2 in range(var1+1, n_vars):\n",
    "                result = _spearmanr_2cols(x[:, [var1, var2]])\n",
    "                rs[var1, var2] = result.correlation\n",
    "                rs[var2, var1] = result.correlation\n",
    "                prob[var1, var2] = result.pvalue\n",
    "                prob[var2, var1] = result.pvalue\n",
    "\n",
    "        res = scipy.stats._stats_py.SignificanceResult(rs, prob)\n",
    "        res.correlation = rs\n",
    "        return res\n",
    "\n",
    "\n",
    "def ic_lgbm(preds: np.narray, train_data: np.narray):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    is_higher_better = True\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import SpearmanCorrCoef\n",
    "\n",
    "tm_spearman = SpearmanCorrCoef()\n",
    "\n",
    "\n",
    "def ic_lgbm(preds: np.ndarray, train_data: np.ndarray):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    global tm_spearman\n",
    "    t_preds = torch.tensor(preds)\n",
    "    t_labels = torch.tensor(train_data.get_label())\n",
    "    print(f\"{t_preds=}\")\n",
    "    print(f\"{t_labels=}\")\n",
    "    t_corr = tm_spearman(t_preds, t_labels)\n",
    "    print(f\"{t_corr=}\")\n",
    "    return 'ic', t_corr.item(), True # is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def ic_lgbm(preds: np.ndarray, train_data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculates the Spearman rank correlation between two tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    x = torch.tensor(preds)\n",
    "    y = torch.tensor(train_data.get_label())\n",
    "    print(f\"{x=}\")\n",
    "    print(f\"{y=}\")\n",
    "    \n",
    "    # Ensure tensors are 1D\n",
    "    x = x.view(-1)\n",
    "    y = y.view(-1)\n",
    "\n",
    "    # Sort the tensors and get their ranks\n",
    "    _, x_ranks = torch.sort(x)\n",
    "    _, y_ranks = torch.sort(y)\n",
    "\n",
    "    # Calculate the Pearson correlation on the ranks\n",
    "    n = x.size(0)\n",
    "    x_ranks = x_ranks.float()\n",
    "    y_ranks = y_ranks.float()\n",
    "    xy_ranks = torch.stack([x_ranks, y_ranks], dim=1)\n",
    "    print(f\"{xy_ranks=}\")\n",
    "    cov = torch.cov(xy_ranks)\n",
    "    std_x = torch.std(x_ranks)\n",
    "    std_y = torch.std(y_ranks)\n",
    "    corr = cov / (std_x * std_y)\n",
    "    print(f\"{corr=}\")\n",
    "    return 'ic', corr.item(), True # is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "cupy.cuda.get_device_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "\n",
    "def spearmanr(preds: np.ndarray, labels: np.ndarray):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    t_preds = cudf.Series(preds)\n",
    "    t_labels = cudf.Series(labels)\n",
    "    return t_preds.corr(t_labels)\n",
    "\n",
    "\n",
    "def ic_lgbm(preds: np.ndarray, train_data: np.ndarray):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    return 'ic', spearmanr(preds, train_data.get_label()), True # is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_preds = np.array(list(range(1.0, 11.0)))\n",
    "# n_labels = np.array(list(range(1.0, 11.0)))\n",
    "n_preds = np.arange(11.0)\n",
    "n_labels = lgb.Dataset(data= np.arange(11.0), label= np.arange(11.0))\n",
    "ic_lgbm(n_preds, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:29.467434Z",
     "start_time": "2021-01-20T05:06:29.459453Z"
    }
   },
   "outputs": [],
   "source": [
    "DAY = 390   # number of minute bars in a trading day of 6.5 hrs (9:30 - 15:59)\n",
    "MONTH = 21  # trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:29.479937Z",
     "start_time": "2021-01-20T05:06:29.468438Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cv(n_splits=23):\n",
    "    return MultipleTimeSeriesCV(n_splits=n_splits,\n",
    "                                lookahead=1,\n",
    "                                test_period_length=MONTH * DAY,       # test for 1 month\n",
    "                                train_period_length=12 * MONTH * DAY,  # train for 1 year\n",
    "                                date_idx='date_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show train/validation periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:55.998459Z",
     "start_time": "2021-01-20T05:06:29.480837Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, (train_idx, test_idx) in enumerate(get_cv().split(X=data)):\n",
    "    train_dates = data.iloc[train_idx].index.unique('date_time')\n",
    "    test_dates = data.iloc[test_idx].index.unique('date_time')\n",
    "    print(train_dates.min(), train_dates.max(), test_dates.min(), test_dates.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T05:06:56.047385Z",
     "start_time": "2021-01-20T05:06:55.999417Z"
    }
   },
   "outputs": [],
   "source": [
    "label = sorted(data.filter(like='fwd').columns)\n",
    "features = data.columns.difference(label).tolist()\n",
    "label = label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T08:46:36.812958Z",
     "start_time": "2021-01-20T08:46:36.804194Z"
    }
   },
   "outputs": [],
   "source": [
    "params = dict(objective='regression',\n",
    "              metric=['rmse'],\n",
    "              device='cpu',\n",
    "              max_bin=63,\n",
    "              gpu_use_dp=False,\n",
    "              num_leaves=16,\n",
    "              min_data_in_leaf=500,\n",
    "              feature_fraction=.8,\n",
    "              verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T08:46:36.812958Z",
     "start_time": "2021-01-20T08:46:36.804194Z"
    }
   },
   "outputs": [],
   "source": [
    "params = dict(objective='quantile',\n",
    "              metric=['quantile', 'rmse'],\n",
    "              device='gpu',\n",
    "              max_depth=4,\n",
    "              num_leaves=16,\n",
    "              # learning_rate=0.1,\n",
    "              # n_estimators=100,\n",
    "              boosting_type='gbdt',\n",
    "              max_bin=63,\n",
    "              gpu_use_dp=False,\n",
    "              min_data_in_leaf=500,\n",
    "              feature_fraction=.8,\n",
    "              verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T08:46:37.104909Z",
     "start_time": "2021-01-20T08:46:37.100050Z"
    }
   },
   "outputs": [],
   "source": [
    "num_boost_round = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T08:46:37.568191Z",
     "start_time": "2021-01-20T08:46:37.566003Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = get_cv(n_splits=23) # we have enough data for 23 different test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T08:46:38.707531Z",
     "start_time": "2021-01-20T08:46:38.705448Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(result):\n",
    "    # print(result['training'].keys())\n",
    "    # print(result['training'])\n",
    "    return pd.DataFrame({'train': result['training']['quantile'],\n",
    "                         'valid': result['valid_1']['quantile']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model-training loop will take more than 10 hours to run and also consumes substantial memory. If you run into resource constraints, you can modify the code, e.g., by:\n",
    "1. Only loading data required for one iteration.\n",
    "2. Shortening the training period to require less than one year.\n",
    "\n",
    "You can also speed up the process by using fewer `n_splits`, which implies longer test periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T17:43:20.892339Z",
     "start_time": "2021-01-20T08:46:39.637329Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(X=data), 1):\n",
    "    # create lgb train set\n",
    "    train_set = data.iloc[train_idx, :]\n",
    "    lgb_train = lgb.Dataset(data=train_set.drop(label, axis=1),\n",
    "                            label=train_set[label],\n",
    "                            categorical_feature=categoricals)\n",
    "    \n",
    "    # create lgb test set\n",
    "    test_set = data.iloc[test_idx, :]\n",
    "    lgb_test = lgb.Dataset(data=test_set.drop(label, axis=1),\n",
    "                           label=test_set[label],\n",
    "                           categorical_feature=categoricals, \n",
    "                           reference=lgb_train)\n",
    "\n",
    "    # train model\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params=params,\n",
    "                      train_set=lgb_train,\n",
    "                      valid_sets=[lgb_train, lgb_test],\n",
    "                      # feval=ic_lgbm,\n",
    "                      num_boost_round=num_boost_round,\n",
    "                      callbacks=[lgb.record_evaluation(evals_result)])\n",
    "                      # callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.record_evaluation(evals_result)])\n",
    "    model.save_model((model_path / f'{fold:02}.txt').as_posix())\n",
    "    \n",
    "    # get train/valid ic scores\n",
    "    scores = get_scores(evals_result)\n",
    "    scores.to_hdf(result_store, f'ic/{fold:02}')\n",
    "    \n",
    "    # get feature importance\n",
    "    fi = get_fi(model)\n",
    "    fi.to_hdf(result_store, f'fi/{fold:02}')\n",
    "    \n",
    "    # generate validation predictions\n",
    "    X_test = test_set.loc[:, model.feature_name()]\n",
    "    y_test = test_set.loc[:, [label]]\n",
    "    y_test['pred'] = model.predict(X_test)\n",
    "    y_test.to_hdf(result_store, f'predictions/{fold:02}')\n",
    "    \n",
    "    # compute average IC per minute\n",
    "    by_minute = y_test.groupby(test_set.index.get_level_values('date_time'))\n",
    "    daily_ic = by_minute.apply(lambda x: spearmanr(x[label], x.pred)[0]).mean()\n",
    "    print(f'\\nFold: {fold:02} | {format_time(time()-start)} | IC per minute: {daily_ic:.2%}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:06:46.180269Z",
     "start_time": "2021-01-20T18:06:39.512596Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(result_store) as store:\n",
    "    pred_keys = [k[1:] for k in store.keys() if k[1:].startswith('pred')]\n",
    "    cv_predictions = pd.concat([store[k] for k in pred_keys]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:06:46.471211Z",
     "start_time": "2021-01-20T18:06:46.181680Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_predictions.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:07:05.478729Z",
     "start_time": "2021-01-20T18:07:05.386471Z"
    }
   },
   "outputs": [],
   "source": [
    "time_stamp = cv_predictions.index.get_level_values('date_time')\n",
    "dates = sorted(np.unique(time_stamp.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have out-of-sample predictions for 484 days from February 2016 through December 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:08:09.303385Z",
     "start_time": "2021-01-20T18:08:09.299517Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'# Days: {len(dates)} | First: {dates[0]} | Last: {dates[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use minutes with at least 100 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:08:19.857439Z",
     "start_time": "2021-01-20T18:08:19.511736Z"
    }
   },
   "outputs": [],
   "source": [
    "n = cv_predictions.groupby('date_time').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ~700 periods, equivalent to a bit over a single trading day (0.67% of all periods in the sample), with fewer than 100 predictions over the 23 test months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:09:25.100235Z",
     "start_time": "2021-01-20T18:09:25.092057Z"
    }
   },
   "outputs": [],
   "source": [
    "incomplete_minutes = n[n<100].index\n",
    "incomplete_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:09:53.749791Z",
     "start_time": "2021-01-20T18:09:53.741585Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{len(incomplete_minutes)} ({len(incomplete_minutes)/len(n):.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:10:25.384858Z",
     "start_time": "2021-01-20T18:10:24.845981Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_predictions = cv_predictions[~time_stamp.isin(incomplete_minutes)]\n",
    "time_stamp = cv_predictions.index.get_level_values('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need at least 10 different pred values in order to bin them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = cv_predictions[\"pred\"].groupby('date_time').nunique()\n",
    "nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_minutesv = nv[nv <= 90].index\n",
    "incomplete_minutesv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(incomplete_minutesv)} ({len(incomplete_minutesv)/len(nv):.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = cv_predictions[~time_stamp.isin(incomplete_minutesv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:10:25.858482Z",
     "start_time": "2021-01-20T18:10:25.572372Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_predictions.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Across all periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:10:39.620370Z",
     "start_time": "2021-01-20T18:10:31.108754Z"
    }
   },
   "outputs": [],
   "source": [
    "ic = spearmanr(cv_predictions.fwd1min, cv_predictions.pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making new predictions every minute, so it makes sense to look at the average performance across all short-term forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:12:59.705706Z",
     "start_time": "2021-01-20T18:12:59.444039Z"
    }
   },
   "outputs": [],
   "source": [
    "minutes = cv_predictions.index.get_level_values('date_time')\n",
    "by_minute = cv_predictions.groupby(minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:12:35.612980Z",
     "start_time": "2021-01-20T18:10:40.873271Z"
    }
   },
   "outputs": [],
   "source": [
    "ic_by_minute = by_minute.apply(lambda x: spearmanr(x.fwd1min, x.pred)[0])\n",
    "\n",
    "minute_ic_mean = ic_by_minute.mean()\n",
    "minute_ic_median = ic_by_minute.median()\n",
    "\n",
    "print(f'\\nAll periods: {ic:6.2%} | By Minute: {minute_ic_mean: 6.2%} (Median: {minute_ic_median: 6.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted as a five-day rolling average, we see that the IC was mostly below the out-of-sample period mean, and increased during the last quarter of 2017 (as reflected in the validation results we observed while training the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T19:57:21.720332Z",
     "start_time": "2021-01-20T19:57:21.327009Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = ic_by_minute.rolling(5*650).mean().plot(figsize=(14, 5), title='IC (5-day MA)', rot=0)\n",
    "ax.axhline(minute_ic_mean, ls='--', lw=1, c='k')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "ax.set_ylabel('Information Coefficient')\n",
    "ax.set_xlabel('')\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized backtest of a naive strategey: financial performance by signal quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphalens does not work with minute-data, so we need to compute our own signal performance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Zipline's Pipeline also doesn't work for minute-data and Backtrader takes a very long time with such a large dataset. Hence, instead of an event-driven backtest of entry/exit rules as in previous examples, we can only create a rough sketch of the financial performance of a naive trading strategy driven by the model's predictions using vectorized backtesting (see Chapter 8 on the [ML4T workflow](../08_ml4t_workflow'). As we will see below, this does not produce particularly helpful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This naive strategy invests in equal-weighted portfolios of the stocks in each decile under the following assumptions (mentioned at the beginning of this notebook: \n",
    "1. Based on the predictions using inputs from the current and previous bars, we can enter positions at the first trade price in the following minute bar\n",
    "2. We exit all positions at the last price in that following minute bar\n",
    "3. There are no trading cost or market impact (slippage) of our trades (but we can check how sensitive the results would be)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average returns by minute bar and signal quantile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we compute the quintiles and deciles of the model's `fwd1min` predictions for each minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:13:30.707553Z",
     "start_time": "2021-01-20T18:13:30.442071Z"
    }
   },
   "outputs": [],
   "source": [
    "by_minute = cv_predictions.groupby(minutes, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_by_minute = cv_predictions['pred'].groupby(minutes, group_keys=False).nunique()\n",
    "nunique_by_minute[nunique_by_minute <= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:18:40.832354Z",
     "start_time": "2021-01-20T18:13:31.167737Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list(range(1, 6))\n",
    "cv_predictions['quintile'] = by_minute.apply(lambda x: pd.qcut(x.pred, q=len(labels), precision=np.inf, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:23:07.357840Z",
     "start_time": "2021-01-20T18:18:40.833391Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list(range(1, 11))\n",
    "cv_predictions['decile'] = by_minute.apply(lambda x: pd.qcut(x.pred, q=len(labels), labels=labels).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T18:23:07.458263Z",
     "start_time": "2021-01-20T18:23:07.364846Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_predictions.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics of intraday returns by quintile and decile of model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the average one-minute returns for each quintile / decile and minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:41:04.915279Z",
     "start_time": "2021-01-20T20:41:04.910687Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_intraday_returns_by_quantile(predictions, quantile='quintile'):\n",
    "    by_quantile = cv_predictions.reset_index().groupby(['date_time', quantile])\n",
    "    return by_quantile.fwd1min.mean().unstack(quantile).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:45:15.737029Z",
     "start_time": "2021-01-20T20:45:09.728192Z"
    }
   },
   "outputs": [],
   "source": [
    "intraday_returns = {'quintile': compute_intraday_returns_by_quantile(cv_predictions),\n",
    "                    'decile': compute_intraday_returns_by_quantile(cv_predictions, quantile='decile')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:45:47.990033Z",
     "start_time": "2021-01-20T20:45:47.987308Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_intraday_returns(returns):\n",
    "    summary = returns.describe(deciles)\n",
    "    return pd.concat([summary.iloc[:1].applymap(lambda x: f'{x:,.0f}'),\n",
    "                      summary.iloc[1:].applymap(lambda x: f'{x:.4%}')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returns per minute, averaged over the 23-months period, increase by quintile/decile and range from -.3 (-.4) to .27 (.37) basis points for the bottom and top quintile (decile), respectively. While this aligns with the finding of a weakly positive rank correlation coefficient, it also suggests that such small gains are unlikely to survive the impact of trading costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:45:51.898922Z",
     "start_time": "2021-01-20T20:45:51.844255Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = summarize_intraday_returns(intraday_returns['quintile'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:45:52.758966Z",
     "start_time": "2021-01-20T20:45:52.662367Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = summarize_intraday_returns(intraday_returns['decile'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Performance by Quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the performance of our naive strategy that trades all available stocks every minute, we simply assume that we can reinvest (including potential gains/losses) every minute. To check for the sensitivity with respect for trading cost, we can assume they are a constant number (fraction) of basis points, and subtract this number from the minute-bar returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:59:03.700181Z",
     "start_time": "2021-01-20T20:59:03.685770Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cumulative_performance(returns, quantile='quintile', trading_costs_bp=0):\n",
    "    \"\"\"Plot average return by quantile (in bp) as well as cumulative return, \n",
    "        both net of trading costs (provided as basis points; 1bp = 0.01%) \n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(14, 4), ncols=2)\n",
    "\n",
    "    sns.barplot(y='fwd1min', x=quantile,\n",
    "                data=returns[quantile].mul(10000).sub(trading_costs_bp).stack().to_frame(\n",
    "                    'fwd1min').reset_index(),\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title(f'Avg. 1-min Return by Signal {quantile.capitalize()}')\n",
    "    axes[0].set_ylabel('Return (bps)')\n",
    "    axes[0].set_xlabel(quantile.capitalize())\n",
    "\n",
    "    title = f'Cumulative Return by Signal {quantile.capitalize()}'\n",
    "    (returns[quantile].sort_index().add(1).sub(trading_costs_bp/10000).cumprod().sub(1)\n",
    "     .plot(ax=axes[1], title=title))\n",
    "\n",
    "    axes[1].yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    axes[1].set_xlabel('')\n",
    "    axes[1].set_ylabel('Return')\n",
    "    fig.suptitle(f'Average and Cumulative Performance (Net of Trading Cost: {trading_costs_bp:.2f}bp)')\n",
    "\n",
    "    sns.despine()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without trading costs, the compounding of even fairly small gains leads to extremely large cumulative profits for the top quantile. However, these disappear as soon as we allow for minuscule trading costs that reduce the average quantile return close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without trading costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:59:10.880167Z",
     "start_time": "2021-01-20T20:59:04.102942Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cumulative_performance(intraday_returns, 'quintile', trading_costs_bp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:59:23.620486Z",
     "start_time": "2021-01-20T20:59:10.881189Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cumulative_performance(intraday_returns, 'decile', trading_costs_bp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With extremely low trading costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:59:30.198103Z",
     "start_time": "2021-01-20T20:59:23.621766Z"
    }
   },
   "outputs": [],
   "source": [
    "# assuming costs of a fraction of a basis point, close to the average return of the top quantile\n",
    "plot_cumulative_performance(intraday_returns, 'quintile', trading_costs_bp=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T20:59:45.543470Z",
     "start_time": "2021-01-20T20:59:30.199046Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cumulative_performance(intraday_returns, 'decile', trading_costs_bp=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a quick look at the features that most contributed to improving the IC across the 23 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T19:07:52.457834Z",
     "start_time": "2021-01-20T19:07:52.362820Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(result_store) as store:\n",
    "    fi_keys = [k[1:] for k in store.keys() if k[1:].startswith('fi')]\n",
    "    fi = pd.concat([store[k].to_frame(i) for i, k in enumerate(fi_keys, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top features from a conventional feature importance perspective are the ticker, followed by NATR, minute of the day, latest 1m return and the CCI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T19:24:49.981841Z",
     "start_time": "2021-01-20T19:24:49.700098Z"
    }
   },
   "outputs": [],
   "source": [
    "fi.mean(1).nsmallest(25).plot.barh(figsize=(12, 8), title='LightGBM Feature Importance (gain)')\n",
    "sns.despine()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore with greater accuracy and in more detail how feature values affect predictions using SHAP values as demonstrated in various other notebooks in this Chapter and the appendix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen that a relatively simple gradient boosting model is able to achieve fairly consistent predictive performance that is significantly better than a random guess even on a very short horizon. \n",
    "\n",
    "However, the resulting economic gains of our naive strategy of frequently buying/(short-)selling the top/bottome quantiles are too small to overcome the inevitable transaction costs. On the one hand, this demonstrates the challenges of extracting value from a predictive signal. On the other hand, it shows that we need a more sophisticated backtesting platform so that we can even begin to design and evaluate a more sophisticated strategy that requires far fewer trades to exploit the signal in our ML predictions. \n",
    "\n",
    "In addition, we would also want to work on improving the model by adding more informative feature, e.g. based on the quote/trade info contained in the Algoseek data, or by fine-tuning our model architecture and hyperparameter settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
